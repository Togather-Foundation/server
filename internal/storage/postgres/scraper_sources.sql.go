// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.30.0
// source: scraper_sources.sql

package postgres

import (
	"context"

	"github.com/jackc/pgx/v5/pgtype"
)

const deleteScraperSource = `-- name: DeleteScraperSource :exec
DELETE FROM scraper_sources WHERE name = $1
`

// Delete a scraper source by name.
func (q *Queries) DeleteScraperSource(ctx context.Context, name string) error {
	_, err := q.db.Exec(ctx, deleteScraperSource, name)
	return err
}

const getScraperSourceByID = `-- name: GetScraperSourceByID :one
SELECT id, name, url, tier, schedule, trust_level, license, enabled,
       max_pages, selectors, notes, last_scraped_at, created_at, updated_at
  FROM scraper_sources
 WHERE id = $1
`

// Get a single scraper source by primary key.
func (q *Queries) GetScraperSourceByID(ctx context.Context, id int64) (ScraperSource, error) {
	row := q.db.QueryRow(ctx, getScraperSourceByID, id)
	var i ScraperSource
	err := row.Scan(
		&i.ID,
		&i.Name,
		&i.Url,
		&i.Tier,
		&i.Schedule,
		&i.TrustLevel,
		&i.License,
		&i.Enabled,
		&i.MaxPages,
		&i.Selectors,
		&i.Notes,
		&i.LastScrapedAt,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getScraperSourceByName = `-- name: GetScraperSourceByName :one
SELECT id, name, url, tier, schedule, trust_level, license, enabled,
       max_pages, selectors, notes, last_scraped_at, created_at, updated_at
  FROM scraper_sources
 WHERE name = $1
`

// Get a single scraper source by unique name.
func (q *Queries) GetScraperSourceByName(ctx context.Context, name string) (ScraperSource, error) {
	row := q.db.QueryRow(ctx, getScraperSourceByName, name)
	var i ScraperSource
	err := row.Scan(
		&i.ID,
		&i.Name,
		&i.Url,
		&i.Tier,
		&i.Schedule,
		&i.TrustLevel,
		&i.License,
		&i.Enabled,
		&i.MaxPages,
		&i.Selectors,
		&i.Notes,
		&i.LastScrapedAt,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const linkOrgScraperSource = `-- name: LinkOrgScraperSource :exec
INSERT INTO org_scraper_sources (organization_id, scraper_source_id)
VALUES ($1, $2)
ON CONFLICT DO NOTHING
`

type LinkOrgScraperSourceParams struct {
	OrganizationID  pgtype.UUID `json:"organization_id"`
	ScraperSourceID int64       `json:"scraper_source_id"`
}

// Associate an organization with a scraper source.
func (q *Queries) LinkOrgScraperSource(ctx context.Context, arg LinkOrgScraperSourceParams) error {
	_, err := q.db.Exec(ctx, linkOrgScraperSource, arg.OrganizationID, arg.ScraperSourceID)
	return err
}

const linkPlaceScraperSource = `-- name: LinkPlaceScraperSource :exec
INSERT INTO place_scraper_sources (place_id, scraper_source_id)
VALUES ($1, $2)
ON CONFLICT DO NOTHING
`

type LinkPlaceScraperSourceParams struct {
	PlaceID         pgtype.UUID `json:"place_id"`
	ScraperSourceID int64       `json:"scraper_source_id"`
}

// Associate a place with a scraper source.
func (q *Queries) LinkPlaceScraperSource(ctx context.Context, arg LinkPlaceScraperSourceParams) error {
	_, err := q.db.Exec(ctx, linkPlaceScraperSource, arg.PlaceID, arg.ScraperSourceID)
	return err
}

const listScraperSources = `-- name: ListScraperSources :many
SELECT id, name, url, tier, schedule, trust_level, license, enabled,
       max_pages, selectors, notes, last_scraped_at, created_at, updated_at
  FROM scraper_sources
 WHERE ($1::boolean IS NULL OR enabled = $1)
 ORDER BY name ASC
`

// List all scraper sources, optionally filtered by enabled flag.
func (q *Queries) ListScraperSources(ctx context.Context, enabled pgtype.Bool) ([]ScraperSource, error) {
	rows, err := q.db.Query(ctx, listScraperSources, enabled)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []ScraperSource{}
	for rows.Next() {
		var i ScraperSource
		if err := rows.Scan(
			&i.ID,
			&i.Name,
			&i.Url,
			&i.Tier,
			&i.Schedule,
			&i.TrustLevel,
			&i.License,
			&i.Enabled,
			&i.MaxPages,
			&i.Selectors,
			&i.Notes,
			&i.LastScrapedAt,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const listScraperSourcesByOrg = `-- name: ListScraperSourcesByOrg :many
SELECT s.id, s.name, s.url, s.tier, s.schedule, s.trust_level, s.license, s.enabled,
       s.max_pages, s.selectors, s.notes, s.last_scraped_at, s.created_at, s.updated_at
  FROM scraper_sources s
  JOIN org_scraper_sources l ON l.scraper_source_id = s.id
 WHERE l.organization_id = $1
 ORDER BY s.name ASC
`

// List all scraper sources linked to a given organization.
func (q *Queries) ListScraperSourcesByOrg(ctx context.Context, organizationID pgtype.UUID) ([]ScraperSource, error) {
	rows, err := q.db.Query(ctx, listScraperSourcesByOrg, organizationID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []ScraperSource{}
	for rows.Next() {
		var i ScraperSource
		if err := rows.Scan(
			&i.ID,
			&i.Name,
			&i.Url,
			&i.Tier,
			&i.Schedule,
			&i.TrustLevel,
			&i.License,
			&i.Enabled,
			&i.MaxPages,
			&i.Selectors,
			&i.Notes,
			&i.LastScrapedAt,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const listScraperSourcesByPlace = `-- name: ListScraperSourcesByPlace :many
SELECT s.id, s.name, s.url, s.tier, s.schedule, s.trust_level, s.license, s.enabled,
       s.max_pages, s.selectors, s.notes, s.last_scraped_at, s.created_at, s.updated_at
  FROM scraper_sources s
  JOIN place_scraper_sources l ON l.scraper_source_id = s.id
 WHERE l.place_id = $1
 ORDER BY s.name ASC
`

// List all scraper sources linked to a given place.
func (q *Queries) ListScraperSourcesByPlace(ctx context.Context, placeID pgtype.UUID) ([]ScraperSource, error) {
	rows, err := q.db.Query(ctx, listScraperSourcesByPlace, placeID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	items := []ScraperSource{}
	for rows.Next() {
		var i ScraperSource
		if err := rows.Scan(
			&i.ID,
			&i.Name,
			&i.Url,
			&i.Tier,
			&i.Schedule,
			&i.TrustLevel,
			&i.License,
			&i.Enabled,
			&i.MaxPages,
			&i.Selectors,
			&i.Notes,
			&i.LastScrapedAt,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const unlinkOrgScraperSource = `-- name: UnlinkOrgScraperSource :exec
DELETE FROM org_scraper_sources
 WHERE organization_id   = $1
   AND scraper_source_id = $2
`

type UnlinkOrgScraperSourceParams struct {
	OrganizationID  pgtype.UUID `json:"organization_id"`
	ScraperSourceID int64       `json:"scraper_source_id"`
}

// Remove an organization↔scraper source association.
func (q *Queries) UnlinkOrgScraperSource(ctx context.Context, arg UnlinkOrgScraperSourceParams) error {
	_, err := q.db.Exec(ctx, unlinkOrgScraperSource, arg.OrganizationID, arg.ScraperSourceID)
	return err
}

const unlinkPlaceScraperSource = `-- name: UnlinkPlaceScraperSource :exec
DELETE FROM place_scraper_sources
 WHERE place_id           = $1
   AND scraper_source_id  = $2
`

type UnlinkPlaceScraperSourceParams struct {
	PlaceID         pgtype.UUID `json:"place_id"`
	ScraperSourceID int64       `json:"scraper_source_id"`
}

// Remove a place↔scraper source association.
func (q *Queries) UnlinkPlaceScraperSource(ctx context.Context, arg UnlinkPlaceScraperSourceParams) error {
	_, err := q.db.Exec(ctx, unlinkPlaceScraperSource, arg.PlaceID, arg.ScraperSourceID)
	return err
}

const updateScraperSourceLastScraped = `-- name: UpdateScraperSourceLastScraped :exec
UPDATE scraper_sources
   SET last_scraped_at = NOW(),
       updated_at      = NOW()
 WHERE name = $1
`

// Update last_scraped_at timestamp after a successful scrape run.
func (q *Queries) UpdateScraperSourceLastScraped(ctx context.Context, name string) error {
	_, err := q.db.Exec(ctx, updateScraperSourceLastScraped, name)
	return err
}

const upsertScraperSource = `-- name: UpsertScraperSource :one

INSERT INTO scraper_sources (
  name, url, tier, schedule, trust_level, license, enabled,
  max_pages, selectors, notes, last_scraped_at, updated_at
) VALUES (
  $1,
  $2,
  $3,
  $4,
  $5,
  $6,
  $7,
  $8,
  $9,
  $10,
  $11,
  NOW()
)
ON CONFLICT (name) DO UPDATE SET
  url             = EXCLUDED.url,
  tier            = EXCLUDED.tier,
  schedule        = EXCLUDED.schedule,
  trust_level     = EXCLUDED.trust_level,
  license         = EXCLUDED.license,
  enabled         = EXCLUDED.enabled,
  max_pages       = EXCLUDED.max_pages,
  selectors       = EXCLUDED.selectors,
  notes           = EXCLUDED.notes,
  last_scraped_at = COALESCE(EXCLUDED.last_scraped_at, scraper_sources.last_scraped_at),
  updated_at      = NOW()
RETURNING id, name, url, tier, schedule, trust_level, license, enabled,
          max_pages, selectors, notes, last_scraped_at, created_at, updated_at
`

type UpsertScraperSourceParams struct {
	Name          string             `json:"name"`
	Url           string             `json:"url"`
	Tier          int32              `json:"tier"`
	Schedule      string             `json:"schedule"`
	TrustLevel    int32              `json:"trust_level"`
	License       string             `json:"license"`
	Enabled       bool               `json:"enabled"`
	MaxPages      int32              `json:"max_pages"`
	Selectors     []byte             `json:"selectors"`
	Notes         pgtype.Text        `json:"notes"`
	LastScrapedAt pgtype.Timestamptz `json:"last_scraped_at"`
}

// SQLc queries for scraper_sources and linkage tables.
// Insert or update a scraper source by name (used by 'server scrape sync').
func (q *Queries) UpsertScraperSource(ctx context.Context, arg UpsertScraperSourceParams) (ScraperSource, error) {
	row := q.db.QueryRow(ctx, upsertScraperSource,
		arg.Name,
		arg.Url,
		arg.Tier,
		arg.Schedule,
		arg.TrustLevel,
		arg.License,
		arg.Enabled,
		arg.MaxPages,
		arg.Selectors,
		arg.Notes,
		arg.LastScrapedAt,
	)
	var i ScraperSource
	err := row.Scan(
		&i.ID,
		&i.Name,
		&i.Url,
		&i.Tier,
		&i.Schedule,
		&i.TrustLevel,
		&i.License,
		&i.Enabled,
		&i.MaxPages,
		&i.Selectors,
		&i.Notes,
		&i.LastScrapedAt,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}
