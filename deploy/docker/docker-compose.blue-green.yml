# Docker Compose configuration for Blue-Green Deployment
# Extends the base docker-compose.yml to add zero-downtime deployment capability
# Reference: specs/001-deployment-infrastructure/spec.md FR-009 (blue-green strategy)

# Blue-Green Deployment Strategy:
# ================================
# Run TWO instances of the app simultaneously: "blue" and "green"
# Only ONE receives traffic at a time via Caddy reverse proxy
#
# Deployment Workflow:
# 1. Deploy new version to inactive slot (e.g., if blue is active, deploy to green)
# 2. Run health checks on new version in the inactive slot
# 3. Switch traffic to new version by updating Caddy reverse_proxy target (atomic cutover)
# 4. Keep old version running briefly for rollback capability
# 5. Stop old version after confirmation new version is stable
#
# Traffic Switching:
# - Controlled by Caddy reverse_proxy configuration (deploy/docker/Caddyfile)
# - Deploy script will handle updating Caddyfile and reloading
# - Rollback is as simple as switching traffic back to the previous slot
#
# Slot Detection:
# - Each app instance knows its slot via SLOT environment variable
# - Used for logging and debugging which version is handling requests

# Import base configuration
# All services defined in base compose file are available here
include:
  - docker-compose.yml

services:
  # Disable the base standalone app service - we only want blue and green slots
  app:
    deploy:
      replicas: 0
  
  # ============================================================================
  # Blue Slot - First Application Instance
  # ============================================================================
  togather-blue:
    # Extend the base app service configuration
    # This inherits all settings: build, environment, volumes, etc.
    extends:
      file: docker-compose.yml
      service: app
    
    # Override container name for blue slot
    container_name: togather-server-blue
    
    # Override port mapping to avoid conflicts
    # Blue slot exposed on host port 8081
    # NOTE: This completely replaces the ports from base service
    ports:
      - "8081:8080"
    
    # Add slot identifier to environment
    environment:
      # All environment variables from base app service are inherited
      # Add slot identifier for logging and debugging
      SLOT: blue
      
      # Override container name in logs
      CONTAINER_NAME: togather-server-blue
      
      # Mark this slot as active (set to "true" for active, "false" for inactive)
      # Change this when switching active slots during deployment
      ACTIVE_SLOT: "${BLUE_ACTIVE_SLOT:-true}"
    
    # Blue and green share the same database
    # Database is NOT duplicated - schema is forward-compatible during deployment
    depends_on:
      togather-db:
        condition: service_healthy
    
    # Add label for identifying slot in docker ps and logs
    labels:
      com.togather.deployment.slot: blue
      com.togather.deployment.strategy: blue-green
    
    # Health check inherited from base app service
    # Deployment script will use this to verify blue slot is ready

  # ============================================================================
  # Green Slot - Second Application Instance
  # ============================================================================
  togather-green:
    # Extend the base app service configuration
    extends:
      file: docker-compose.yml
      service: app
    
    # Override container name for green slot
    container_name: togather-server-green
    
    # Override port mapping to avoid conflicts
    # Green slot exposed on host port 8082
    # NOTE: This completely replaces the ports from base service
    ports:
      - "8082:8080"
    
    # Add slot identifier to environment
    environment:
      # All environment variables from base app service are inherited
      # Add slot identifier for logging and debugging
      SLOT: green
      
      # Override container name in logs
      CONTAINER_NAME: togather-server-green
      
      # Mark this slot as active (set to "true" for active, "false" for inactive)
      # Change this when switching active slots during deployment
      ACTIVE_SLOT: "${GREEN_ACTIVE_SLOT:-false}"
    
    # Blue and green share the same database
    depends_on:
      togather-db:
        condition: service_healthy
    
    # Add label for identifying slot
    labels:
      com.togather.deployment.slot: green
      com.togather.deployment.strategy: blue-green
    
    # Health check inherited from base app service

  # ============================================================================
  # Caddy Reverse Proxy - Traffic Router
  # ============================================================================
  # Routes all incoming traffic to either blue or green slot
  # Traffic switching is controlled by Caddy reverse_proxy configuration
  caddy:
    image: caddy:2-alpine
    container_name: togather-proxy
    
    # Public-facing port
    # All client traffic comes through this port
    ports:
      - "${PROXY_PORT:-80}:80"
    
    # Network connectivity
    networks:
      - togather-net
    
    # Mount Caddy configuration
    # Configuration file specifies which slot receives traffic
    volumes:
      # Read-only mount to prevent accidental modification
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      # Caddy data directory for certificates and state
      - caddy-data:/data
      - caddy-config:/config
    
    # Proxy depends on both slots being available
    # This ensures slots are running before proxy starts
    depends_on:
      togather-blue:
        condition: service_healthy
      togather-green:
        condition: service_healthy
    
    # Health check for proxy itself
    healthcheck:
      # Check that Caddy is responding and can reach upstream
      test: ["CMD-SHELL", "wget -q --spider http://localhost:80/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    
    # Restart policy
    restart: unless-stopped
    
    # Labels for management
    labels:
      com.togather.deployment.role: proxy
      com.togather.deployment.strategy: blue-green
    
    # Resource limits (proxy is lightweight)
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 32M

  # ============================================================================
  # Monitoring Services (Optional - controlled by ENABLE_MONITORING env var)
  # ============================================================================
  
  # Prometheus - Metrics Collection and Storage
  prometheus:
    image: prom/prometheus:v3.5.1
    container_name: togather-prometheus
    
    # Only start if monitoring is enabled
    profiles:
      - monitoring
    
    # Command-line arguments
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    
    # Ports (9090 internal only, can be exposed for debugging)
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    
    # Networks
    networks:
      - togather-net
    
    # Mount configuration and data volumes
    volumes:
      # Prometheus configuration
      - ../config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      # Persistent metrics storage
      - prometheus-data:/prometheus
    
    # Health check
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    # Restart policy
    restart: unless-stopped
    
    # Labels
    labels:
      com.togather.service: monitoring
      com.togather.component: prometheus
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
  
  # Grafana - Metrics Visualization and Dashboards
  grafana:
    image: grafana/grafana:12.3.2
    container_name: togather-grafana
    
    # Only start if monitoring is enabled
    profiles:
      - monitoring
    
    # Ports (3000 exposed for web UI access)
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    
    # Networks
    networks:
      - togather-net
    
    # Environment variables
    environment:
      # Admin credentials
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      
      # Server settings
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:3000}
      GF_SERVER_SERVE_FROM_SUB_PATH: "false"
      
      # Anonymous access (disabled by default)
      GF_AUTH_ANONYMOUS_ENABLED: ${GRAFANA_ANONYMOUS_ENABLED:-false}
      
      # Datasource provisioning
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      
      # Disable telemetry
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      GF_ANALYTICS_CHECK_FOR_UPDATES: "false"
    
    # Mount configuration and data volumes
    volumes:
      # Datasource provisioning
      - ../config/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
      # Dashboard provisioning (includes dashboards.yml config and JSON files)
      - ../config/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      # Persistent Grafana data (users, settings, etc.)
      - grafana-data:/var/lib/grafana
    
    # Depends on Prometheus being available
    depends_on:
      prometheus:
        condition: service_healthy
    
    # Health check
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    # Restart policy
    restart: unless-stopped
    
    # Labels
    labels:
      com.togather.service: monitoring
      com.togather.component: grafana
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

# Networks configuration inherited from base docker-compose.yml
# All services (togather-db, blue, green, caddy, prometheus, grafana) share the togather-net network

# Volumes configuration
volumes:
  # Caddy data directory (certificates and persistent state)
  caddy-data:
    driver: local
    labels:
      com.togather.service: proxy
      com.togather.component: caddy
  
  # Caddy config directory
  caddy-config:
    driver: local
    labels:
      com.togather.service: proxy
      com.togather.component: caddy
  
  # Prometheus persistent storage for metrics
  prometheus-data:
    driver: local
    labels:
      com.togather.service: monitoring
      com.togather.component: prometheus
  
  # Grafana persistent storage for dashboards and settings
  grafana-data:
    driver: local
    labels:
      com.togather.service: monitoring
      com.togather.component: grafana

# Usage Instructions:
# ===================
#
# 1. Start both slots with Caddy proxy:
#    docker compose -f docker-compose.yml -f docker-compose.blue-green.yml up -d
#
# 2. Check which slot is active:
#    cat deploy/docker/Caddyfile | grep "reverse_proxy togather-server-"
#
# 3. Deploy new version to inactive slot:
#    - If blue is active, update green:
#      docker compose -f docker-compose.blue-green.yml up -d --no-deps togather-green
#    - If green is active, update blue:
#      docker compose -f docker-compose.blue-green.yml up -d --no-deps togather-blue
#
# 4. Run health checks on updated slot:
#    curl http://localhost:8081/health  # Blue slot
#    curl http://localhost:8082/health  # Green slot
#
# 5. Switch traffic to updated slot:
#    - Update Caddyfile to point to the new slot
#    - Reload Caddy: docker compose -f docker-compose.blue-green.yml exec caddy caddy reload --config /etc/caddy/Caddyfile
#
# 6. Verify traffic is flowing to new slot:
#    curl -I http://localhost/health | grep X-Togather-Slot
#    docker compose -f docker-compose.blue-green.yml logs -f togather-green  # or togather-blue
#
# 7. After confirmation, stop old slot (optional):
#    docker compose -f docker-compose.blue-green.yml stop togather-blue  # or togather-green
#
# Rollback:
# =========
# If new version has issues, switch traffic back to old slot:
# 1. Update Caddyfile to point to old slot
# 2. Reload Caddy: docker compose -f docker-compose.blue-green.yml exec caddy caddy reload --config /etc/caddy/Caddyfile
# 3. Traffic is now back on the stable version (zero downtime)
#
# Notes:
# ======
# - Both slots share the same database (single source of truth)
# - Database migrations must be forward-compatible (old code runs against new schema during cutover)
# - Deploy script will automate steps 3-6 above
# - Manual testing can be done by accessing slots directly on ports 8081/8082
# - Production deployments should use the deploy script for safety checks

# Usage Instructions for Monitoring Stack:
# ========================================
#
# The monitoring stack uses Docker Compose profiles to enable optional services.
# By default, Prometheus and Grafana are NOT started.
#
# To start with monitoring enabled:
#   docker compose -f docker-compose.yml -f docker-compose.blue-green.yml --profile monitoring up -d
#
# To start without monitoring (default):
#   docker compose -f docker-compose.yml -f docker-compose.blue-green.yml up -d
#
# Access URLs (when monitoring enabled):
#   - Prometheus UI: http://localhost:9090
#   - Grafana UI:    http://localhost:3000 (default credentials: admin/admin)
#
# Environment Variables:
#   - PROMETHEUS_PORT:          Port for Prometheus UI (default: 9090)
#   - GRAFANA_PORT:             Port for Grafana UI (default: 3000)
#   - GRAFANA_ADMIN_USER:       Grafana admin username (default: admin)
#   - GRAFANA_ADMIN_PASSWORD:   Grafana admin password (default: admin)
#   - GRAFANA_ROOT_URL:         Grafana root URL (default: http://localhost:3000)
#   - GRAFANA_ANONYMOUS_ENABLED: Enable anonymous access (default: false)
#
# Prometheus Configuration:
#   - Scrape interval: 15 seconds
#   - Retention: 15 days or 10GB (whichever comes first)
#   - Scrape targets: togather-blue:8080/metrics, togather-green:8080/metrics
#
# Grafana Configuration:
#   - Datasource: Prometheus at http://prometheus:9090 (auto-provisioned)
#   - Dashboards: Auto-loaded from /etc/grafana/provisioning/dashboards
#   - Persistent data: Stored in grafana-data volume
#
# Notes:
#   - Prometheus and Grafana data persists across container restarts
#   - Change the default Grafana admin password in production!
#   - Prometheus can reach both blue and green slots via Docker network
